---
title: "Writing Assignment"
author: "Seth Harrison"
bibliography: [packages.bib, ISLR.bib]
output: 
    bookdown::html_document2
date: 'Last compiled: `r format(Sys.time(), "%b %d, %Y")`'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA)
```

## Directions{-} 
Recreate this document exactly using [R Markdown](http://rmarkdown.rstudio.com/).  A great reference for creating technical documents with R Markdown is [_bookdown: Authoring Books and Technical Documents with R Markdown_](https://bookdown.org/yihui/bookdown/).  Your YAML should look similar to:

```{r, eval = FALSE}
---
title: "Writing Assignment"
author: "Leave This Blank"
bibliography: [packages.bib, ISLR.bib]
output: 
    bookdown::html_document2
date: 'Last compiled: `r format(Sys.time(), "%b %d, %Y")`'
---
```


# From page 62 of [ISLR](http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf) [@james_introduction_2013] 
Let $\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1x_i$ be the prediction for $Y$ based on the $i^{\text{th}}$ value of $X$.  Then $e_i = y_i - \hat{y}_i$ represents the $i^{\text{th}}$ _residual_---this is the difference between the $i^{\text{th}}$ observed response and the $i^{\text{th}}$ response value that is predicted by our linear model.  We define the _residual sum of squares_ (RSS) as

\begin{equation*}
\mathrm{RSS} = e_1^2 + e_2^2 + \cdots + e_n^2.
\end{equation*}

or equivalently as

\begin{equation}
\mathrm{RSS} = (y_1 - \hat{\beta}_0 - \hat{\beta}_1x_1 )^2 + (y_2 - \hat{\beta}_0 - \hat{\beta}_1x_2 )^2 + \cdots + (y_n - \hat{\beta}_0 - \hat{\beta}_1x_n )^2
(\#eq:rss)
\end{equation}

The least squares approach chooses $\hat{\beta}_0$ and $\hat{\beta}_1$ to minimize using the RSS. Using some calculus, one can show that the minimizers are

\begin{equation}
\begin{split}
\hat{\beta}_1 &= \frac{\sum_{i=1}^n(x_i - \bar{x})(y_i - \bar{y}) }{\sum_{i=1}^n (x_i - \bar{x})^2},\\
\hat{\beta}_0 &= \bar{y} - \hat{\beta}_1\bar{x},\\
\end{split}
(\#eq:alpha)
\end{equation}

where $\bar{y} \equiv \frac{1}{n} \sum_{i=1}^n y_i$ and $\bar{x} \equiv \frac{1}{n} \sum_{i=1}^nx_i$ are the sample means.

#From page 63 of [ISLR](http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf) [@james_introduction_2013]

Recall that we assume that the true relationship between $X$ and $Y$ takes the form $Y=f(X)+ϵ$ for some unknown function $f$, where ϵ is a mean-zero random error term. If $f$ is to be approximated by a linear function, then we write this relationship as

\begin{equation}
Y = \beta_0 + \beta_1 + \epsilon 
(\#eq:beta)
\end{equation}

Here $β0$ is the intercept term—that is, the expected values of $Y$ when $X=0$, and $β1$ is the slope—the average increase in $Y$ associated with a one-unit increase in $X$. The error term is a catch-all for what we miss with this simple model: the true relationship is probably not linear, there may be other variables that cause variation in $Y$, and there may be measurement error. We typically assume that the error term is independent of $X$.

#From page 143 of [ISLR](http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf) [@james_introduction_2013]

To indicate that a p-dimensional random variable $X$ has a multivariate Gaussian distribution, we write $X∼N(μ,Σ)$. Here $E(X)=μ$ is the mean of $X$ (a vector with p components), and $COV(X)=Σ$ is the $p×p$ covariance matrix of $X$. Formally, the multivariate Gaussian density is defined as

\begin{equation}
f(x) = \frac{1}{(2\pi)^{p/2}| \mathbf{\Sigma}|^{1/2}} \text{exp} \left(-\frac{1}{2}(x - \mu)^T \mathbf{\Sigma}^{-1} (x - \mu) \right)
(\#eq:omega)
\end{equation}

In the case of $p>1$ predictors, the LDA classifier assumes that the observations in the $k^{th}$ class are drawn from a multivariate Gaussian distribution $N(μ_k,\mathbf\Sigma)$, where $μ_k$ is a class-specific mean vector, and $\mathbf\Sigma$ is the covariance matrix that is common to all $K$ classes. Plugging the density function for the $k^{th}$ class, $f_k(X=x)$, into (3.1) and performing a little bit of algebra reveals that the Bayes classifier assigns an observation $X=x$ to the class for which

\begin{equation}
\delta_k(x) = x^{T} \mathbf\Sigma^{-1} \mu_k - \frac{1}{2} \mu^{T}_k \mathbf\Sigma^{-1} \mu_k + \log \pi_k
(\#eq:epsilon)
\end{equation}

is the largest.

# Inserting a graph

```{r, label = "fig4", fig.cap = "Your descriptive caption here", fig.align='center'}
set.seed(123)
x <- rnorm(1000, 100, 15)
DF <- data.frame(x = x)
library(ggplot2)
ggplot(data = DF, aes(x = x)) + 
  geom_histogram(fill = "blue", color = "black", binwidth = 5) + 
  theme_bw()
```

```{r}
xbar <- mean(x)
SD <- sd(x)
c(xbar, SD)
```

Figure \@ref(fig:fig4) is unimodal with a mean of `r mean(x)` and a standard deviation of `r sd(x)`

#Automagically Creating References

Review your last assignment to create a file named `packages.bib` to cite the `ggplot2` package used to create Figure \@ref(fig:fig4). Figure \@ref(fig:fig4) was created with `ggplot2` by @R-ggplot2. This document specifies the output as `bookdown::html_document2`. The function `bookdown::html_document2` is from `bookdown` written by @R-bookdown.

```{r, results = "hide", echo = FALSE, warning = FALSE}
packages <- c("ggplot2", "bookdown")
knitr::write_bib(packages, file = "./packages.bib")
lapply(packages, library, character.only = TRUE)
```

```{r}
sessionInfo()
```

# References